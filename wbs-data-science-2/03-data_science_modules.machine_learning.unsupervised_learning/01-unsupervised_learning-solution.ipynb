{"metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Unsupervised Learning"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As a first step, we will need to import the data from the \"`online_retail.csv`\" data file that was preprocessed for us so that features are properly scaled.\n", "\n", "We also want to define the column that we are going to use as the row labels of the DataFrame; in this case, *'CustomerID'*. Once loaded, we can apply once more the `head()` function to preview the first 5 rows of our DataFrame. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import the data from the online_retail.csv, \n", "# set the index column and explore the first few rows\n", "customers = pd.read_csv('data/online_retail.csv', index_col='CustomerID')\n", "customers.head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clustering with K-Means\n", "\n", "K-means clustering is a method for finding clusters and cluster centers in a set of points. The K-means algorithm alternates the two steps:\n", "\n", "1. for each center, identify the subset of training points that are closer to it than to any other center\n", "2. update the location of the center to match the points related to it\n", "\n", "These two steps are iterated until the centers no longer move (significantly) or the assignments no longer change. Then, a new point $x$ can be assigned to the cluster of the closest prototype.\n", "\n", "### Learning Activity - Run K-Means with two features\n", "\n", "Isolate the features `mean_spent` and `max_spent`, then run the K-Means algorithm on the resulting dataset using K=2 and visualise the result. You will need:\n", "\n", "* to create an instance of `KMeans` with 2 clusters\n", "* fit this to the isolated features (via the `.fit` method)\n", "* look how it's doing by using by showing the assignment predicted (via the `.predict` method)\n", "\n", "This is the standard SKLearn workflow for most of the algorithms."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\n", "\n", "# Apply k-means with 2 clusters using a subset of features \n", "# (mean_spent and max_spent)\n", "kmeans = KMeans(n_clusters = 2)\n", "cust2  = customers[['mean_spent', 'max_spent']]\n", "kmeans.fit(cust2)\n", "cluster_assignment = kmeans.predict(cust2)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us introduce a simple function to better visualise what's going on:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# This function generates a pairplot enhanced with the result of k-means\n", "def pairplot_cluster(df, cols, cluster_assignment):\n", "    \"\"\"\n", "    Input\n", "        df, dataframe that contains the data to plot\n", "        cols, columns to consider for the plot\n", "        cluster_assignments, cluster asignment returned \n", "        by the clustering algorithm\n", "    \"\"\"\n", "    # seaborn will color the samples according to the column cluster\n", "    df['cluster'] = cluster_assignment \n", "    sns.pairplot(df, vars=cols, hue='cluster', size=6)\n", "    df.drop('cluster', axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And let's use it now to see how we did previously... (ignore the warnings if anything comes up)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise the clusters using pairplot_cluster()\n", "pairplot_cluster(customers, ['mean_spent', 'max_spent'], cluster_assignment)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### What can you observe?\n", "\n", "* the separation between the two clusters is \"clean\" (the two clusters can be separated with a line)\n", "* one cluster contains customers with low spendings, the other one with high spendings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Test Activity - Run K-Means with all the features\n", "Run K-Means using all the features available and visualise the result in the subspace `mean_spent` and `max_spent`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Apply k-means with 2 clusters using all features\n", "kmeans = KMeans(n_clusters = 2)\n", "kmeans.fit(customers)\n", "cluster_assignment = kmeans.predict(customers)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["and visualise using the same subset of variables as before... what has changed??"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise the clusters using pairplot_cluster()\n", "pairplot_cluster(customers, ['mean_spent', 'max_spent'], cluster_assignment)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***Question***: Why can't the clusters be separated with a line as before?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning activity - Compare expenditure between clusters\n", "\n", "Select the features `'mean_spent'` and `'max_spent'` and compare the two clusters obtained above using them. \n", "\n", "To do so, filter `customers` to keep only the rows associated to a given cluster, then apply `.describe()` on the result. That will return a DataFrame object containing summary statistics, you can save it in a new variable. If you do so for both clusters, you can then concatenate the two variables in one single DataFrame and easily compare the statistics per cluster."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare expenditure between clusters\n", "features = ['mean_spent', 'max_spent']\n", "\n", "# create a dataframe corresponding to the case\n", "# cluster_assignment == 0\n", "cluster_0 = customers.loc[cluster_assignment == 0, features].describe()\n", "cluster_0.columns = [c + '_0' for c in cluster_0.columns]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# then with ==1\n", "cluster_1 = customers.loc[cluster_assignment == 1, features].describe()\n", "cluster_1.columns = [c + '_1' for c in cluster_1.columns]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Concatenate both:\n", "compare_df = pd.concat([cluster_0, cluster_1], axis=1)\n", "compare_df\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Test Activity - Looking at the centroids\n", "\n", "Look at the centroids of the clusters `kmeans.cluster_centers_` and check the values of the centers in for the features `mean_spent`, `max_spent`. You will need to create a new DataFrame where the data is simply `kmeans.cluster_centers_`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get the centroids and display them\n", "centers_df = pd.DataFrame(data=kmeans.cluster_centers_, columns=customers.columns)\n", "print(centers_df[features])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning Activity - Compare mean expediture with box plot\n", "\n", "Compare the distribution of the feature `mean_spent` in the two clusters using a box plot. You will need:\n", "\n", "* `sns.boxplot` (seaborn's boxplot)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare mean expediture with box plot\n", "sns.boxplot(data=pd.DataFrame({\"mean_spent_0\": customers.loc[cluster_assignment == 0, \"mean_spent\"], \n", "                               \"mean_spent_1\": customers.loc[cluster_assignment == 1, \"mean_spent\"]}), \n", "            showfliers=False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["does this seem to make sense? How can you interpret the plots?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning Activity - Compute the silhouette score\n", "Compute the silhouette score of the clusters resuting from the application of K-Means.\n", "\n", "The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``. It represents how similar a sample is to the samples in its own cluster compared to samples in other clusters.\n", "\n", "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n", "\n", "SKLearn provides the function `silhouette_score` which you can call and display."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import silhouette_score\n", "\n", "# Computing the silhouette score\n", "print('silhouette_score {0:.2f}'.format(silhouette_score(customers, cluster_assignment)))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This silhouette score is reasonably high which we can intepret by saying that the corresponding clusters are quite compact."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hierarchical clustering: Linking with Linkage\n", "\n", "The main idea behind hierarchical clustering is that you start with each point in it's own cluster and then\n", "\n", "1. compute distances between all clusters\n", "2. merge the closet clusters\n", "\n", "Do this repeatedly until you have only one cluster.\n", "\n", "This algorithm groups the samples in a bottom-up fashion and falls under the category of the agglomerative clustering algorithms.\n", "\n", "According to the distance between clusters and samples that one choose the clusters will have different properties. In this section we'll use a distance that will minimizes the variance of the clusters being merged.\n", "\n", "This algorithm results in a hierarchy, or binary tree, of clusters branching down to the last layer which has a leaf for each point in the dataset that can be visualise with a \"Dendrogram\". The advantage of this approach is that clusters can grow according to the shape of the data rather than being globular.\n", "\n", "sklearn implements hierarchical clustering in the class `sklearn.cluster.AgglomerativeClustering` (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering), this class is mainly a wrapper to the functions in `scipy.cluster.hierarchy` (http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n", "\n", "### Learning Activity - Plotting dendograms\n", "Use the function `linkage()` from `scipy.cluster.hierarchy` to cluster the retail data and pass the result to the function `dendrogram()` to visualise the result. Truncate the dendrogram if the initial result is unreadable."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.cluster.hierarchy import linkage, dendrogram\n", "\n", "# Apply hierarchical clustering \n", "Z = linkage(customers, method='ward', metric='euclidean')\n", "\n", "# Draw the dendrogram\n", "plt.figure(figsize=(18,6))\n", "dendrogram(Z)\n", "plt.ylabel('Distance')\n", "plt.xlabel('Sample index')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The coloring of the figure highlights that the data can be segmented in two big clusters that were merged only in the very last iterations of the algorithm.\n", "\n", "We can improve the readability of the dendrogram showing only the last merged clusters and a threshold to color the clusters. For this use\n", "\n", "* the option `truncate_mode` in `dendrogram`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Draw the dendrogram using a cut_off value\n", "plt.figure(figsize=(18,6))\n", "cut_off = 60\n", "\n", "dendrogram(Z, color_threshold=cut_off, truncate_mode='lastp', p=20)\n", "\n", "plt.ylabel('Distance')\n", "plt.xlabel('Sample index or (cluster size)')\n", "plt.hlines(cut_off, 0, len(customers), linestyle='--')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["oh yeah that's a lot better!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning Activity - Running Linkage with Sklearn\n", "\n", "Use `sklearn.cluster.AgglomerativeClustering` to cluster the retail data according to the 3 clusters highlighted by the dendrogram above and visualise the result in the subspace give by the features `mean_spent` and `max_spent`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import AgglomerativeClustering\n", "\n", "# Perform clustering with AgglomerativeClustering\n", "# Ward merges two clusters if the resulting has small variance\n", "\n", "n_cluster = 3\n", "agglomerative = AgglomerativeClustering(\n", "                    n_clusters=n_cluster, linkage='ward', \n", "                    affinity='euclidean')\n", "\n", "cluster_assignment = agglomerative.fit_predict(customers)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's visualise this with the `pairplot_cluster()`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise the clusters using pairplot_cluster()\n", "pairplot_cluster(customers, ['mean_spent', 'max_spent'], cluster_assignment)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DBSCAN\n", "\n", "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are globular. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster. \n", "\n", "Summary of the Algorithm:\n", "\n", "- starts with an arbitrary starting point and retrieved all the points in the radius of distance `eps` from it \n", "    - if the radius contains `min_samples` points, start a cluster\n", "      - add all the points in the radius of distance `eps` to the cluster and their `eps` neighbors.\n", "      - continue expanding the cluster iterating on the the procedure on all the neighbors\n", "    - otherwise mark it as noise/outlier\n", "\n", "Sklearn implementation doc: http://scikit-learn.org/stable/modules/clustering.html#dbscan\n", "\n", "Animated DBSCAN: http://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Learning Activity - A starting value for eps\n", "\n", "Measure the distance of each point to its closest neighbor using the function `sklearn.metrics.pairwise.pairwise_distances` (http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) and plot the distribution of the distances."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics.pairwise import pairwise_distances\n", "\n", "# Compute all the pairwise distances\n", "all_distances = pairwise_distances(customers, metric='euclidean')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# compute the distance of each point to its closest neighbor\n", "neig_distances = [np.min(row[np.nonzero(row)]) for row in all_distances]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the distances\n", "plt.hist(neig_distances, bins=50)\n", "plt.xlabel('Distance from closest sample')\n", "plt.ylabel('Occurrences')\n", "plt.axis([0,1.5,0,2500])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The distribution of the distance will help us choose a starting point for `eps`. We see that it's very likely that a point as at least one neighbour in a radius of 0.15 and that only very few point have one at distance over 1.0. Since we want that a core point has more than one point in is `eps`-neighborhood we can start picking `eps` on the right tail of the distribution."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Learning Activity - Applying DBSCAN\n", "\n", "Cluster the customer data with DBSCAN and visualise the results in the subspaces used for the other algorithms."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import DBSCAN\n", "\n", "# Apply DBSCAN setting eps to 1.0 and min samples to 8 (say)\n", "db = DBSCAN(eps=1.0, min_samples=8)\n", "cluster_assignment = db.fit_predict(customers)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display how many clusters were found\n", "clusters_found = np.unique(cluster_assignment)\n", "print ('Clusters found', len(clusters_found))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can then visualise this"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise the clusters using pairplot_cluster()\n", "pairplot_cluster(customers, ['mean_spent', 'max_spent'], cluster_assignment)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["DBSCAN clustered all the points in one big cluster and marked as outiers all the points that are not in dense areas."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning Activity -  How many clusters with DBSCAN?\n", "\n", "Vary `eps` and `min_samples` and study how the number of clusters varies as result. This way we'll have an idea of how many cluster we get varying the parameters. This can help us choose the parameters if we already have an idea of how many clusters we want to create.\n", "\n", "Warning, if you cover a grid of points, it may take a while to finish, don't put too many points!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# WARNING this may take a couple of minutes to finish!\n", "eps = np.linspace(.5, 10.0, 5)\n", "mins = np.arange(5, 50, 5)\n", "Z = np.zeros((len(eps), len(mins)))\n", "\n", "for i, e in enumerate(eps):\n", "    for j, m in enumerate(mins):\n", "        db   = DBSCAN(eps=e, min_samples=m)\n", "        pred = db.fit_predict(customers)\n", "        clusters_found = len(np.unique(pred))\n", "        Z[i,j] = clusters_found"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualise this using a heatmap\n", "plt.figure(figsize=(10, 10))\n", "sns.heatmap(Z, cmap='RdBu', center=0, annot=True);\n", "plt.xticks(np.arange(Z.shape[1]), mins)\n", "plt.xlabel('min_samples')\n", "plt.yticks(np.arange(Z.shape[0]), ['%0.2f' % x for x in eps])\n", "plt.ylabel('eps')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Learning activity - Compute the silhouette score of the DBSCAN cluster\n", "\n", "Compute the silhouette score of the clusters made with DBSCAN and compare it with the silhouette score achieved with K-Means."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute the silhouette score of DBSCAN\n", "print(silhouette_score(customers, cluster_assignment))\n"]}]}