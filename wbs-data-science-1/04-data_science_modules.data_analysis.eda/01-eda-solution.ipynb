{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Pre-processing\n",
    "\n",
    "In this module you will learn how to load an online retail dataset in Python, visualise and summarise it to produce insights that will guide your machine learning endevours in the next modules. \n",
    "\n",
    "You will learn how to plot data and calculate summary statistics to build a dataset from which you ultimately will try to predict future customer behaviour.\n",
    "\n",
    "For this module, you will need to import \n",
    "\n",
    "* `numpy` (as `np`)  [link to documentation](https://docs.scipy.org/doc/)\n",
    "* `pandas` (as `pd`) [link to documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* `matplotlib.pyplot` (as `plt`) [link to documentation](https://matplotlib.org/contents.html)\n",
    "* `seaborn` (as `sns`) [link to documentation](https://seaborn.pydata.org)\n",
    "\n",
    "\n",
    "In order to use `matplotlib` in the notebook, recall that you need to have a cell with\n",
    "```python\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here to load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n",
    "Load the dataset `online_retail_customer_data.csv` with `pandas`. Recall that you need to \n",
    "\n",
    "* use the `read_csv()` method from `pandas`\n",
    "* point to the location of the dataset\n",
    "* determine a name under which you want to store the resulting data frame (we suggest the name `customers`)\n",
    "* specify that the `CustomerID` column is the index column using the `index_col` option\n",
    "\n",
    "Use the `head` method to display the first few lines of the dataset (you can specify how many lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('data/online_retail_customer_data.csv',\n",
    "                        index_col='CustomerID')\n",
    "\n",
    "customers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### About the dataset\n",
    "\n",
    "The dataset is based on data from an online retailer selling gifts and is based on a dataset taken from [here](https://archive.ics.uci.edu/ml/datasets/Online+Retail#).\n",
    "\n",
    "We have taken the original data set and processed it to create a 'profile' for each customer, which includes a number of features such as:\n",
    "\n",
    "* `Country`: The country the purchases were made from.\n",
    "* `balance`: Amount of money spent at the store (purchases minus refunds).\n",
    "* `n_orders`: Total number of orders from the online retailer.\n",
    "* `time_between_orders`: Average time (in days) between orders.\n",
    "* `max_spent`: Most amount of money customer spent on a single order.\n",
    "\n",
    "Check the dimensionality of the dataset using the `shape` attribute of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code to check the dimensions of the dataset\n",
    "customers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "At this stage you do not really know what is going on in this dataset. \n",
    "You need to go beyond the first impression by considering simple questions like:\n",
    "\n",
    "* How many customers are you dealing with?\n",
    "* What country spends how much?\n",
    "* What has been the company's profit during the last year?\n",
    "* Are there differences between customers that return and those who don't?\n",
    "\n",
    "You will go through these questions and learn new tricks as you move along. The first one is easy to answer, you can use the `nunique()` method applied to the index column. You can retrieve the index column using the `index` attribute of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cca": {
     "exercise": false
    }
   },
   "outputs": [],
   "source": [
    "# use the nunique method\n",
    "customers.index.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That could have been expected!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of features and One-Hot-Encoding\n",
    "\n",
    "Data can be both \n",
    "\n",
    "* on a *continuous* scale: e.g.: the amount of money spent in the store or time between purchases\n",
    "* on a *discrete* scale: e.g.: the country.\n",
    "\n",
    "Discrete variables that have a notion of ordering (for instance, a survey that asks your satisfaction from scale of one to five) are called *ordinal*. Discrete variables that cannot be ordered are usually referred to as *categorical* variables (eg: countries or gender).\n",
    "\n",
    "In the feature engineering step, one typically needs to pay special attention to discrete variables as many models are not equipped to handle this type of data, particularly if they are just categorical.\n",
    "\n",
    "In the case where categorical features are present, you need to represent them as numerical values. \n",
    "A standard approach to do so is the **one-hot encoding**. \n",
    "The input in one-hot encoding is the vector of discrete categorical values, and the output is a sparse matrix where each column corresponds to one possible value of the feature.\n",
    "As an example, let's consider the following trivial dataset\n",
    "\n",
    "```\n",
    "Nick, UK\n",
    "Laura, IT\n",
    "Massimo, IT\n",
    "```\n",
    "\n",
    "In this case, there are two countries `[\"UK\", \"IT\"]`, the one-hot-encoding would correspond to the table\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|cc}\n",
    "& \\text{UK} & \\text{IT} \\\\\\hline\n",
    "\\text{Nick} & 1 & 0\\\\\n",
    "\\text{Laura} &0& 1\\\\\n",
    "\\text{Massimo}&0 & 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In order to do that on the original dataset:\n",
    "\n",
    "* Use the function `get_dummies()` from `pandas` on the series `Country` from `customers`\n",
    "* Apply `set_index(customers.index)` to the corresponding object so it has the same index as our original dataframe. This is so that you'll be able to join them later on. (*Can you explain what this command does?*)\n",
    "* Save the new dataframe in a variable `countries`\n",
    "* use `head()` to have a look and make sure it all makes sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.get_dummies(customers.Country).set_index(customers.index)\n",
    "countries.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now drop the original `Country` column from the `customers` dataframe and keep this one. For this, use `pandas`' `drop()` function. Don't forget to use the `inplace` parameter if you want your change to be appied on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.drop('Country', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do pre-processing and imputation on the continuous features, so let's keep this separate for now and join it together later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values and Imputation\n",
    "\n",
    "You may have noticed that there are missing values in the data (`NaN`).\n",
    "It's very important in general to check whether there are any and\n",
    "\n",
    "* whether these missing values are informative or not\n",
    "* whether you can replace the missing values in a sensible way or not\n",
    "\n",
    "First, check which column has missing values and how many.\n",
    "For this, use\n",
    "\n",
    "* the `isnull()` method applied on the data frame, this returns a dataframe similar to the original one but where every entry is just `True` or `False`\n",
    "* on the resulting dataframe, apply the `sum()` method which will count how many entries of the column are `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do you think might be the reason why some customers have missing values as their time between orders?*\n",
    "\n",
    "In general, for columns with missing values, there are a few choices on how to handle them. \n",
    "This process is usually called *imputation*.\n",
    "\n",
    "#### Imputation \n",
    "\n",
    "There are many strategies to help with missing data and they depend on whether the missing data is numeric or categorical. Recall that you can for example\n",
    "\n",
    "* simply remove rows where there is missing data (e.g. `.dropna()` can achieve this)\n",
    "* imputing the values with a summary statistic such as mean or median or most frequent value (e.g. `Imputer` from `sklearn` module)\n",
    "* replace the values with a sensible estimate\n",
    "\n",
    "What strategy is best for you problem very much depends on the specifics of your dataset. \n",
    "\n",
    "In the current case, the missing values are exclusively found in the `time_between_orders` column, so you should have a look at these rows where this occurs to see if we can gain an understanding of what may be causing these missing values.\n",
    "\n",
    "* select the customers for which `time_between_orders` is null. For this, use `isnull` on the appropriate column and feed it as row indices to the dataframe to retrieve a subdataframe only corresponding to those customers\n",
    "* check the shape, make sure it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here to find the instances where time_between_orders is empty\n",
    "msk_null_time = customers.time_between_orders.isnull()\n",
    "null_time = customers[msk_null_time]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many nan cases do you have? have a look\n",
    "null_time.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `n_orders` seems to often be equal to `1`. \n",
    "There is a fairly obvious interpretation for those, since they haven't yet come back, there is no \"time between orders\".\n",
    "You can count the number of time specific values of `n_orders` occur by using the `value_counts()` method applied on the series corresponding to `n_orders`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_time.n_orders.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of missing values can therefore be explained by customers that have not returned before.\n",
    "It is unclear at this point what the remaining 200 are.\n",
    "\n",
    "This dataset is of course fairly artificial so we won't discuss this in too much detail at this point. \n",
    "\n",
    "In this case, we decide to fill the missing values by 365 days by using the `fillna()` function (this is a fairly reckless decision but, again, this notebook is more focused on tools and techniques)\n",
    "\n",
    "* replace the column `time_between_orders` by the same column where the missing values are filled with value 365 using the `fillna()` function applied on the column\n",
    "* use `head()` to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.time_between_orders.fillna(value=365, inplace=True)\n",
    "customers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "\n",
    "Outliers are observation that appear extreme relative to the bulk of the data.\n",
    "Machine Learning techniques can be sensitive to outliers. \n",
    "\n",
    "Here you will see how you can get rid of them if that's what you decide to do. \n",
    "There are multiple ways to define outliers, one possibility is to consider all points that are more than `k` standard deviations (`sigma`, $\\sigma$) away from the mean (`mu`, $\\mu$) of the data.\n",
    "\n",
    "Below you can see a simple function that takes data and a number of standard deviations and filters out everything that doesn't lie in the range $[\\mu-k\\sigma, \\mu+k\\sigma]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a Series and filters out all elements that are outside\n",
    "# the range [mu-k*sigma , mu+k*sigma]\n",
    "def remove_outliers(data, k=3):\n",
    "    mu       = data.mean() # get the mean\n",
    "    sigma    = data.std()  # get the standard deviation\n",
    "    filtered = data[(mu-k*sigma < data) & (data < mu+k*sigma)]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `.apply()` this function on your dataframe. It will call it on each column individually and will return a new dataframe where values declared as outliers are replaced by `NaN`, keeping the structure of the `pd.DataFrame` intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.apply(remove_outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the lines with `NaN` values (that correspond to lines with outliers). For this, use the `dropna()` method on the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dropna(inplace=True)\n",
    "customers.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "The different numerical variables have completely different scales. This becomes even more obvious when considering a boxplot. You'll use the `seaborn` wrapper around `matplotlib` that is great for producing clear plots.\n",
    "Have a look [here](https://stanford.edu/~mwaskom/software/seaborn/examples/index.html) for a gallery of plots possible with `seaborn`.\n",
    "\n",
    "* define a figure environment with the `figure()` method of `matplotlib.pyplot` (you can pass a figure size)\n",
    "* use the `boxplot` function of `seaborn` specifying the appropriate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code to plot a sns.boxplot() of the customer dataframe\n",
    "plt.figure(figsize=(17,7))\n",
    "sns.boxplot(data=customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `n_orders` is defined in a much narrower space than `balance`. \n",
    "If you were to use the data in an unscaled form, the effect of `balance` might be disproportionnaly high and cause a Machine Learning algorithm to underperform. \n",
    "\n",
    "To account for this, it is good practice to scale your data, so that all the dimensions fall onto a comparable interval. To do this:\n",
    "\n",
    "* Define a \"scaler\" using the `StandardScaler` class imported from `sklearn.preprocessing` (you could also use the `MinMaxScaler`)\n",
    "* Apply it on the dataframe using the `fit_transform` method \n",
    "* Define a new dataframe similar to the original one but with scaled columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here to apply the scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "customers = pd.DataFrame(data=scaler.fit_transform(customers),\n",
    "                              columns = customers.columns,\n",
    "                              index = customers.index)\n",
    "customers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the `boxplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here to replot the boxplot with the scaled data\n",
    "plt.figure(figsize=(17,7))\n",
    "sns.boxplot(data=customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between input features\n",
    "\n",
    "An important tool for the exploratory data analysis step is the **scatter plot**. \n",
    "\n",
    "This plot helps visualise the relationship in-between two input features. It may also give you a first indication of the Machine Learning model that could be applied and its complexity (linear vs. non-linear). \n",
    "\n",
    "Create a scatter plot of the `n_orders` vs `balance` using the `lmplot` function of `seaborn`. Once you have a grip of this, you should try looking at the scatter plot corresponding to other couples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('n_orders', 'balance', customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) - Grid/scatterplot matrix\n",
    "\n",
    "A scatterplot matrix shows a grid of all scatterplots where each attribute is plotted against all other attributes.\n",
    "This can be applied when there aren't too many variables (otherwise it quickly becomes impractical). \n",
    "\n",
    "You can find further information on how to create a scatterplot matrix with seaborn using the `pairplot()` function [here](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(customers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix and heatmap of correlations between the input features\n",
    "\n",
    "It is often of great interest to investigate whether any of the variables in a multivariate dataset are significantly correlated. \n",
    "As previously shown, the different features (variables) in `customers` are not independent from each other. \n",
    "To quickly identify which features are related and to what degree, it is useful to compute a correlation matrix that shows the correlation coefficient for each pair of variables. \n",
    "You can do this by using the `corr()` function from the `pandas` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the degree of correlation between variables, you can use a heatmap (also from `seaborn`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(customers.corr(), center=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance thresholding\n",
    "\n",
    "Variance thresholding is a simple approach that removes features with low variance, in order to reduce dimensionality while preserving as much of the variation expressed by the data as possible/practical.\n",
    "\n",
    "Importantly, this thresholding does not take any classification into account, so we are examining the variance for a given feature across samples, not the variance relative to any output or class.\n",
    "\n",
    "Variance thresholding is implemented as a transformer object in `scikit-learn` with a number of different options.\n",
    "\n",
    "* Join the `countries` dataframe corresponding to the one-hot encoding to the `customers` dataframe\n",
    "* create an object of the `VarianceThreshold` class from `sklearn.feature_selection` to select the subset of features with variance of at least `0.5`\n",
    "* run the `fit()` method on the object, you can then use the `get_support()` method to see an array of True/False for which columns pass the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here for the variance thresholding \n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "customers = pd.merge(customers, countries,\n",
    "                     left_index=True, right_index=True)\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=0.5)\n",
    "thresholder.fit(customers)\n",
    "customers.columns[thresholder.get_support()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Which features would be removed based on this strategy?\n",
    "What are the drawbacks of this method?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
