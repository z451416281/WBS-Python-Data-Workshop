{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Bagging - Random Forest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Note**: If you have completed the Decision Tree notebook already, those preprocessing steps are the same. Feel free to copy paste answers from the previous notebook or the solutions and jump straight to the Random Forest part."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The dataset can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/bank+marketing). It consists of data from marketing campaigns of a Portuguese bank. We will try to build a classifier that can predict whether or not the client targeted by the campaign ended up subscribing to a term deposit (column `y`).\n", "\n", "Load the file `data/bank-marketing.zip` with pandas and check the distribution of the target `y`. Here the separator is `';'` instead of a comma."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The dataset is imbalanced, we will need to keep that in mind when building our models!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now split the data into the feature matrix `X` (all features except `y`) and the target vector `y` making sure that you convert `yes` to `1` and `no` to `0`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get X, y\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here is the list of features in our X matrix:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "1. age (numeric)\n", "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n", "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n", "4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n", "5. default: has credit in default? (categorical: 'no','yes','unknown')\n", "6. housing: has housing loan? (categorical: 'no','yes','unknown')\n", "7. loan: has personal loan? (categorical: 'no','yes','unknown')\n", "8. contact: contact communication type (categorical: 'cellular','telephone') \n", "9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n", "10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n", "11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n", "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n", "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n", "14. previous: number of contacts performed before this campaign and for this client (numeric)\n", "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n", "16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n", "17. cons.price.idx: consumer price index - monthly indicator (numeric) \n", "18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n", "19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n", "20. nr.employed: number of employees - quarterly indicator (numeric)\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note the comment about the `duration` feature. We will exclude it from our analysis.\n", "\n", "Drop `duration` from X:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can check the types of all our features. We see that some seem to be categorical whilst others are numerical. We will keep a two lists, one for each type, so we can preprocess them differently."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["X.dtypes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# they have a third class \"unknown\" we'll process them as non binary categorical\n", "num_features = [\"age\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \n", "                \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]\n", "\n", "cat_features = [\"job\", \"marital\", \"education\",\"default\", \"housing\", \"loan\",\n", "                \"contact\", \"month\", \"day_of_week\", \"poutcome\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualise the numerical features\n", "\n", "* show a boxplot of the numerical features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The features aren't at the same scale. But it's all fine for tree based methods as we've seen in the course, so we do not need to do any scaling here!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### One Hot Encoding on Categorical Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order to make sure our dataset contains only number we will need to transform our categorical features into one hot encoded features. To do so, first, use `pd.get_dummies` on your dataframe (select only the categorical features) to generate the new columns. Assign the new dataframe to a variable `X_categorical`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Create, now we can create `X_processed` using `pd.concat` (check the documentation, you will need to specify the right axis). Here we want to concatenate a dataframe with only our numerical features together with our `X_categorical` we created above:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Split data into training set and test set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split your data (use `X_processed`) into training and test set. Here we are dealing with an imbalanced dataset, so it is important to enforce stratification. We will use the argument `stratify` from `train_test_split` to do so (check the documentation)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, we're ready to start training our random forest!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Random Forest"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have preprocessed our data, we can train a Random Forest on it. For that, we will import the `RandomForestClassifier` from `sklearn.ensemble`\n", "\n", "For now we will make our Random Forest bad on purpose by deactivating some important parameters to better see their impact.\n", "\n", "Create a new Random Forest where the `RandomForestClassifier` has the following parameters:\n", "- `max_depth`=3, \n", "- `min_samples_split`=.1\n", "- `n_estimators`=15\n", "- `max_features`=None\n", "- ` bootstrap`=False\n", "\n", "We will explain the role of those parameters step by step in this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Train your model on the training set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check the performances of our newly trained model on the test set. Compute the accuracy score and display the classification report. Both can be found in the package `sklearn.metrics`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["It's actually not that bad. First thing we notice is that we have much lower performance scores on the class `1`. That's because we do not have many observations in that class so the model focuses on class `0`. We can fix that by using the parameter `class_weight=\"balanced\"`. Use `set_params` to set the class weight of our model to balanced:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Train your model again and verify it has improved the performance of class `1` by printing the classification  report:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Better. But let's take a closer look at our ensemble to see if we're doing things right. First thing we can do is access single trees in our forest and take a look at their individual performances.\n", "\n", "You can access the list of trees in your ensemble with the attribute `.estimators_` on the random forest.\n", "\n", "Extract the two first trees in your ensemble and call them `dt1` and `dt2`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now print the classification report for each of the two trees:\n", "\n", "Note: You can call `.predict` on a single tree to generate a prediction."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The two reports seem **extremely** similar. Something does not look right. Let's plot the two trees to debug a bit further.Execute the two cells below to display then. What can you see?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.display import Image  \n", "from sklearn import tree\n", "import pydotplus\n", "\n", "dot_data = tree.export_graphviz(dt1, \n", "                                out_file=None, \n", "                                filled=True, \n", "                                rounded=True,\n", "                                max_depth=6,\n", "                                proportion=True,\n", "                                feature_names=X_train.columns,\n", "                                special_characters=True)  \n", "\n", "graph = pydotplus.graph_from_dot_data(dot_data)  \n", "Image(graph.create_png())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dot_data = tree.export_graphviz(dt2, \n", "                                out_file=None, \n", "                                filled=True, \n", "                                rounded=True,\n", "                                max_depth=6,\n", "                                proportion=True,\n", "                                feature_names=X_train.columns,\n", "                                special_characters=True)  \n", "\n", "graph = pydotplus.graph_from_dot_data(dot_data)  \n", "Image(graph.create_png())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It looks like are trees are exactly the same. There are three reasons for that:\n", "\n", "- We have no used any bootstrap (`bootstrap`=False)\n", "- We have not usre any feature subsampling (`max_features`=None)\n", "- We have built too simple trees (`max_depth` and `min_samples_split`)\n", "\n", "By doing so, we are not benefiting at all from the boost in performance that bagging should bring."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now use `set_params` to set `bootstrap=True` and `max_features=\"auto\"`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Train your model again:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the accuracy and classification report on the test set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Interesting, our performance have actually decreased here. That's because we have introduce more noise by adding boostraping and feature subsampling, so indivual trees are more varied... but by construction are less good than the optimal tree we built earlier. In general this wouldn't be an issue if trees are complex enough to compensate each other's error. Here our trees are two constrained.\n", "\n", "First, let's visualise our two first trees as we did before. Execute the cells below to do so (you might have to change the names of variables if you have used different ones). Are our trees different?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dt1 = rf.estimators_[0]\n", "dt2 = rf.estimators_[1]\n", "\n", "print(classification_report(y_test, dt1.predict(X_test)))\n", "print(classification_report(y_test, dt2.predict(X_test)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dot_data = tree.export_graphviz(dt1, \n", "                                out_file=None, \n", "                                filled=True, \n", "                                rounded=True,\n", "                                max_depth=6,\n", "                                proportion=True,\n", "                                feature_names=X_train.columns,\n", "                                special_characters=True)  \n", "\n", "graph = pydotplus.graph_from_dot_data(dot_data)  \n", "Image(graph.create_png())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dot_data = tree.export_graphviz(dt2, \n", "                                out_file=None, \n", "                                filled=True, \n", "                                rounded=True,\n", "                                max_depth=6,\n", "                                feature_names=X_train.columns,\n", "                                proportion=True,\n", "                                special_characters=True)  \n", "\n", "graph = pydotplus.graph_from_dot_data(dot_data)  \n", "Image(graph.create_png())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next step is to find the best trade off for `max_depth` and `min_samples_split` to have trees performant enough but also varied enough. For this we will use grid search:\n", "\n", "Define a new grid search object checking a few sensible values for `max_depth` and `min_samples_split` and train it on the training set.\n", "\n", "Note: Keep in mind here that you want your trees to overfit a little, so do not constrain them too much"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["What are your best parameters?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now re-train your model using those parameters:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Print accuracy and classification report:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["That's getting better! Now let's increase the number of trees by setting `n_estimators`, try to increase it until you do not get a performance boost anymore.\n", "\n", "What's your final accuracy?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot feature importance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since Random Forest relies on Decision Trees, we can access features importance as well. Here the features importance will just be averaged over our trees.\n", "\n", "With sklearn, you can access it with the attribute `feature_importances_`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, now create a new dataframe where the data is the feature importances you saved above, and the index will be the list of columns from X_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot it as a bar plot:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["What can you observe? What are your main features? \n", "\n", "Compare with the features importance with decision tree alone."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optional: how would we optimise for recall?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grid Search by default will return the parameters that give the best accuracy. But what if we cared more about the recall?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can overwrite the metrics that the grid search is using when comparing two models. The code below will convert the `recall_score` function into a scorer object using `make_scorer`. The resulting object can be passed as `scoring` argument to the gridsearch."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import recall_score, make_scorer\n", "\n", "gridCV = GridSearchCV(rf, parameters, cv=10, n_jobs=-1, scoring=make_scorer(recall_score))\n", "\n", "gridCV.fit(X_train, y_train)\n", "gridCV.best_params_"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["rf.set_params(**gridCV.best_params_)\n", "rf.fit(X_train,y_train)\n", "\n", "print(accuracy_score(y_test, rf.predict(X_test)))\n", "print(classification_report(y_test, rf.predict(X_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We've decreased in overall accuracy, but managed to increase the recall (for class 1 by default) a bit more!"]}]}